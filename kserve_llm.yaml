apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: qwen2-5
spec:
  predictor:
    model:
      args:
      - --model_name=Qwen2.5-72B-Instruct-GPTQ-Int4
      - --model_id=/mnt/models/Qwen2.5-72B-Instruct-GPTQ-Int4
      - --gpu_memory_utilization=0.7
      - --max-model-len=32768
      image: kserve/huggingfaceserver:v0.14.0
      modelFormat:
        name: huggingface
      protocolVersions: v2
      resources:
        limits:
          cpu: "10"
          memory: 50Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "10"
          memory: 50Gi
          nvidia.com/gpu: "1"
      volumeMounts:
      - mountPath: /mnt/models
        name: llm
        readOnly: true
    volumes:
    - name: llm
      persistentVolumeClaim:
        claimName: llm
        readOnly: true


---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama3
spec:
  predictor:
    model:
      args:
      - --model_name=Meta-Llama-3-8B-Instruct
      - --model_id=/mnt/models/meta-llama/Meta-Llama-3-8B-Instruct
      - --gpu-memory-utilization=0.28
      - --max-model-len=500
      image: kserve/huggingfaceserver:v0.14.0
      modelFormat:
        name: huggingface
      protocolVersions: v2
      resources:
        limits:
          cpu: "10"
          memory: 50Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "10"
          memory: 50Gi
          nvidia.com/gpu: "1"
      volumeMounts:
      - mountPath: /mnt/models
        name: llm
        readOnly: true
    volumes:
    - name: llm
      persistentVolumeClaim:
        claimName: llm
        readOnly: true
