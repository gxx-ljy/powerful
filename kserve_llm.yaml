apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: qwen2-5
spec:
  predictor:
    model:
      args:
      - --model_name=Qwen2.5-72B-Instruct-GPTQ-Int4
      - --model_id=/mnt/models/Qwen2.5-72B-Instruct-GPTQ-Int4
      - --gpu_memory_utilization=0.7
      - --max-model-len=32768
      image: kserve/huggingfaceserver:v0.14.0
      modelFormat:
        name: huggingface
      protocolVersions: v2
      resources:
        limits:
          cpu: "10"
          memory: 50Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "10"
          memory: 50Gi
          nvidia.com/gpu: "1"
      volumeMounts:
      - mountPath: /mnt/models
        name: llm
        readOnly: true
    volumes:
    - name: llm
      persistentVolumeClaim:
        claimName: llm
        readOnly: true
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test
  namespace: ns-profile
spec:
  ingressClassName: nginx
  rules:
    - host: ai.com
      http:
        paths:
          - backend:
              service:
                name: qwen2-5-predictor-00001-private
                port:
                  number: 80
            path: /openai/v1/chat
            pathType: ImplementationSpecific
---
def llm_response(prompt, content):
    import json
    import requests

    data = {
        "model": "Qwen2.5-72B-Instruct-GPTQ-Int4",
        "messages": [
            {
                "role": "system",
                "content": prompt
            },
            {
                "role": "user",
                "content": content
            }
        ],
        "max_tokens": 20000,
        "temperature": 0,
        # "stream": False
    }

    # response = json.loads(requests.post(url = "http://qwen2-5.ns-profile.svc.cluster.local/openai/v1/completions", json=data).text)
    response = requests.post(url = "https://ai.com/openai/v1/chat/completions", json=data).json()

    result = response#['choices'][0]['message']['content']
    return result
llm_response("hello", "你是谁")
---
!curl -v http://qwen2-5.ns-profile.svc.cluster.local/openai/v1/completions \
-H "content-type: application/json" -H "Host: qwen2-5.ns-profile.svc.cluster.local" \
-d '{"model": "Qwen2.5-72B-Instruct-GPTQ-Int4", "prompt": "Write a poem about colors", "stream":false, "max_tokens": 30}'
---
!curl -v http://llama3.ns-profile.svc.cluster.local/openai/v1/completions \
-H "content-type: application/json" -H "Host: llama3.ns-profile.svc.cluster.local" \
-d '{"model": "Meta-Llama-3-8B-Instruct", "prompt": "Write a poem about colors", "stream":false, "max_tokens": 30}'
---
!curl -H "content-type:application/json" -H "Host: llama3.ns-profile.svc.cluster.local" \
-v http://llama3.ns-profile.svc.cluster.local/openai/v1/chat/completions \
-d '{"model":"Meta-Llama-3-8B-Instruct","messages":[{"role":"system","content":"You are an assistant that speaks like Shakespeare."},{"role":"user","content":"Write a poem about colors"}],"max_tokens":30,"stream":false}'

---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama3
spec:
  predictor:
    model:
      args:
      - --model_name=Meta-Llama-3-8B-Instruct
      - --model_id=/mnt/models/meta-llama/Meta-Llama-3-8B-Instruct
      - --gpu-memory-utilization=0.28
      - --max-model-len=500
      image: kserve/huggingfaceserver:v0.14.0
      modelFormat:
        name: huggingface
      protocolVersions: v2
      resources:
        limits:
          cpu: "10"
          memory: 50Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: "10"
          memory: 50Gi
          nvidia.com/gpu: "1"
      volumeMounts:
      - mountPath: /mnt/models
        name: llm
        readOnly: true
    volumes:
    - name: llm
      persistentVolumeClaim:
        claimName: llm
        readOnly: true
